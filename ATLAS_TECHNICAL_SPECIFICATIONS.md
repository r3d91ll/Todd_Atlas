# Atlas Technical Specifications
**Extracted from arXiv:2505.23735v1**

## 1. Memory Update Equations

### Core Atlas Update Rule (Omega Rule with Muon)

```
‚Ñ≥‚Çú = Œ±‚Çú‚Ñ≥‚Çú‚Çã‚ÇÅ - Œ∑‚Çú NS-5(ùíÆ‚Çú)
ùíÆ‚Çú = Œ∏‚ÇúùíÆ‚Çú‚Çã‚ÇÅ - ‚àá‚Ñì(‚Ñ≥‚Çú‚Çã‚ÇÅ; ùê§‚Çú, ùêØ‚Çú)
```

### Loss Objective

```
min_‚Ñ≥ Œ£·µ¢‚Çå‚Çú‚Çã‚Çí‚Çä‚ÇÅ·µó Œ≥·µ¢‚ÅΩ·µó‚Åæ ||‚Ñ≥(œï(ùê§·µ¢)) - ùêØ·µ¢||¬≤‚ÇÇ
```

### Variable Definitions

- **‚Ñ≥‚Çú**: Memory state at time step t (learnable memory matrix/MLP)
- **Œ±‚Çú**: Dynamic decay coefficient (input-dependent, not fixed)
- **Œ∑‚Çú**: Learning rate (adaptive schedule)
- **NS-5**: Normalized Symmetric rank-5 approximation (from Muon optimizer)
- **ùíÆ‚Çú**: Momentum accumulator (stores gradient history)
- **Œ∏‚Çú**: Momentum decay coefficient
- **‚àá‚Ñì**: Gradient of loss with respect to memory parameters
- **ùê§‚Çú, ùêØ‚Çú**: Key-value pairs at time t
- **Œ≥·µ¢‚ÅΩ·µó‚Åæ ‚àà [0,1]**: Input-dependent context gates (learned pruning weights)
- **œï(¬∑)**: Polynomial feature mapping of degree p
- **c**: Context window length (number of past tokens to memorize)
- **i**: Time index within window [t-c+1, t]

### Polynomial Feature Mapping

```
œï‚Çö(x) = [x·µù]_{|Œ≤|‚â§p}
```

Where Œ≤ represents multi-index notation for polynomial terms up to degree p.

**Capacity Implications**:
- Linear memory + Hebbian: O(d‚Çñ) capacity
- With polynomial features: **O(d‚Çñ·µñ) capacity**
- Deep MLP memory: O(d‚Çñd·µ•Œ£·µ¢‚Çå‚ÇÅ^ùìõ_‚Ñ≥ min{d‚Çï‚ÅΩ ≤‚Åæ}‚±º‚â•·µ¢ d‚Çï‚ÅΩ ≤‚Å∫¬π‚Åæ)

## 2. What Atlas Adds vs. Titans/Miras

### Compared to Titans

**Titans**: First-order gradient descent for memory updates
**Atlas**: Approximated second-order optimization via Muon optimizer

**Key Innovation**: "Locally optimal" memory management through NS-5 transformation, which approximates Hessian information without explicit computation. This prevents convergence to spurious local minima in the memory optimization landscape.

### Compared to Miras Framework

**Miras**: Optimizes single (ùê§‚Çú, ùêØ‚Çú) pairs (online learning)
**Atlas**: Optimizes over entire context windows

**Key Innovation**: "Test-time memorization of context" rather than individual tokens. The loss function sums over window [t-c+1, t], enabling the model to learn contextual relationships rather than isolated associations.

### Unique Atlas Features

From Table 1 ablation comparison:
1. **Dynamic decay** (Œ±‚Çú is input-dependent)
2. **Deep neural memory** (MLP with ‚â•1 layers + residuals)
3. **Non-linear capacity** (polynomial feature expansion)
4. **Locally optimal** (second-order approximation via Muon)
5. **Flexible context** (learned window pruning via Œ≥·µ¢ gates)

### Architectural Distinction

**Sliding Window Attention (SWA)**: Dense attention masks over fixed windows
**Atlas**: Learned sparse context selection through gradient-based optimization of Œ≥·µ¢‚ÅΩ·µó‚Åæ gates, enabling "in-context token pruning without increasing parameters proportionally"

## 3. Memory Initialization

### Deep Memory Networks

Atlas uses **standard deep initialization** for MLP-based memory:
- MLPs with ‚â•1 hidden layers
- Residual connections for gradient flow
- Standard Xavier/He initialization for weight matrices

### Polynomial Feature Coefficients

For polynomial feature mapping œï‚Çö(x):

```
a·µ¢ = 1/i!
```

**Rationale**: Approximates Taylor expansion of exponential kernels, providing a theoretically-grounded initialization for polynomial features.

### Momentum Accumulator

```
ùíÆ‚ÇÄ = 0  (zero initialization)
```

Standard practice for momentum-based optimizers.

### Context Gates

Œ≥·µ¢‚ÅΩ·µó‚Åæ gates are learned parameters, likely initialized near 1.0 to preserve full context initially, then learned through backpropagation.

## 4. Learning Rate Handling

### Adaptive Learning Rate Schedule

**Œ∑‚Çú**: Adaptive (time-varying) learning rate

The paper mentions "adaptive learning rate schedule" but does not specify exact schedule (likely cosine annealing or similar, common in large-scale training).

### Muon Optimizer Integration

The NS-5 transformation acts as a **preconditioner** on gradients:

```
Œ∑‚Çú NS-5(ùíÆ‚Çú)
```

This combines:
- Global learning rate Œ∑‚Çú
- Local curvature information from NS-5
- Momentum accumulation from ùíÆ‚Çú

### Second-Order Approximation

Muon's NS-5 provides "Hessian-free second-order updates" - the effective learning rate is modulated by approximated curvature without explicit Hessian computation.

**Benefit**: More stable convergence in non-convex memory optimization landscape.

## 5. Training Tricks & Stability

### Numerical Stability

1. **Momentum accumulation** smooths gradient updates, preventing oscillations
2. **Polynomial feature expansion** maintains bounded intermediate values through factorial initialization (a·µ¢ = 1/i!)
3. **Decay coefficients** (Œ±‚Çú) keep memory values normalized over time
4. **NS-5 rank-5 approximation** limits dimensionality of second-order information, avoiding full Hessian costs

### Parallelization Strategy

**Section 3.3 key insight**: Unlike online updates (c=1), context windows enable **batch gradient computation**:

```
"Fast training without substantial overhead compared to the online version"
```

**Implementation**: Compute gradients for multiple tokens in window [t-c+1, t] simultaneously, then aggregate before memory update.

**Trade-off**: Enables parallelism while avoiding quadratic attention costs of full Transformers.

### Context Window Selection

```
c ‚àà ‚Ñï‚â•‚ÇÅ
```

- **c=1**: Reduces to online Delta rule (sequential, no context)
- **c=context_length**: Global optimization (memory-intensive)
- **Intermediate c**: Balances expressivity and computational efficiency

**Design choice**: Flexible c allows tuning memory-compute trade-off per deployment scenario.

### Input-Dependent Context Pruning

**Œ≥·µ¢‚ÅΩ·µó‚Åæ gates**: Learned sparse attention weights within context window

**Benefit**: "In-context token pruning without increasing parameters proportionally" - model learns which historical tokens are relevant for current update, avoiding dense computation.

## 6. Key Architectural Insights

### 1. Locally Optimal Memory Management

**Problem**: First-order gradient descent can converge to spurious local minima in memory optimization.

**Atlas Solution**: Approximated second-order information (Muon/NS-5) provides curvature awareness, enabling "locally optimal" updates that better navigate the non-convex loss surface.

**Impact**: More stable and effective memory consolidation during both training and test-time adaptation.

### 2. Context vs. Token Memorization

**Previous approaches (Titans/Miras)**: Optimize individual (ùê§‚Çú, ùêØ‚Çú) pairs sequentially.

**Atlas**: Optimizes over context windows Œ£·µ¢‚Çå‚Çú‚Çã‚Çí‚Çä‚ÇÅ·µó, enabling **relational memory** rather than isolated associations.

**Insight**: "Test-time memorization of context" allows the model to learn dependencies between tokens within a window, improving coherence and factual accuracy.

### 3. Capacity Through Non-Linearity

**Linear memory**: O(d‚Çñ) capacity (limited by key dimensionality)

**Atlas with polynomial features**: **O(d‚Çñ·µñ) capacity**

**Insight**: Non-linear feature expansion (polynomial œï‚Çö) exponentially increases memory capacity without proportional parameter growth. This is a **fundamentally different scaling law** than attention-based architectures.

### 4. Fixed-Size State vs. Growing KV Cache

**Transformer**: KV cache grows linearly with sequence length (O(n¬∑d))

**Atlas**: Fixed-size memory state (O(d¬≤) for matrix, O(Œ£d‚Çï) for MLP)

**Insight**: Atlas maintains **constant memory footprint** regardless of sequence length, making it suitable for long-context scenarios where Transformer KV caches become prohibitive.

### 5. Learned Sparse Context

**Dense attention**: O(n¬≤) computation over all pairs

**Atlas**: O(c¬∑d¬≤) computation with learned sparsity via Œ≥·µ¢ gates

**Insight**: Rather than hand-crafting attention patterns (sliding windows, strided patterns), Atlas **learns** which context tokens matter through gradient-based optimization. This combines flexibility of full attention with efficiency of sparse patterns.

### 6. Gradient-Based Test-Time Adaptation

**Standard inference**: Fixed weights, no adaptation

**Atlas**: Memory parameters ‚Ñ≥ continue optimizing during inference via gradient descent on context windows

**Insight**: "Test-time memorization" enables the model to **adapt to distribution shifts** and **incorporate new information** without retraining. This is a form of meta-learning baked into the architecture.

### 7. Omega Rule as Principled Framework

**Ad-hoc memory updates**: Various heuristics (EMA, Hebbian, etc.)

**Atlas Omega Rule**: Derived from explicit loss minimization with momentum

**Insight**: Provides theoretical grounding for memory update mechanisms. The Omega rule isn't a heuristic - it's the **gradient descent solution** to the context memorization objective with momentum.

## Implementation Considerations

### Inference Requirements

1. **Cache past keys**: Store ùê§·µ¢ for i ‚àà [t-c+1, t]
2. **Fixed memory state**: ‚Ñ≥‚Çú (constant size)
3. **Momentum buffer**: ùíÆ‚Çú (same size as ‚Ñ≥)
4. **Context gates**: Œ≥·µ¢‚ÅΩ·µó‚Åæ values (c scalars per position)

**Memory footprint**: O(c¬∑d‚Çñ + |‚Ñ≥| + |ùíÆ|), where |‚Ñ≥| is memory parameter count.

### Training Requirements

1. **Batch gradient computation**: Compute ‚àá‚Ñì for all i ‚àà [t-c+1, t] in parallel
2. **Momentum accumulation**: Update ùíÆ‚Çú ‚Üê Œ∏‚ÇúùíÆ‚Çú‚Çã‚ÇÅ - Œ£‚àá‚Ñì·µ¢
3. **NS-5 transformation**: Apply Muon's rank-5 approximation to ùíÆ‚Çú
4. **Memory update**: ‚Ñ≥‚Çú ‚Üê Œ±‚Çú‚Ñ≥‚Çú‚Çã‚ÇÅ - Œ∑‚Çú NS-5(ùíÆ‚Çú)

**Computational cost**: O(c¬∑d¬≤) per update (c gradient computations, each O(d¬≤) for memory parameters).

### Hyperparameters to Tune

1. **c**: Context window length (impacts memory-compute trade-off)
2. **p**: Polynomial feature degree (impacts capacity vs. dimensionality)
3. **Œ∑‚Çú schedule**: Learning rate decay (impacts adaptation speed)
4. **Œ∏‚Çú**: Momentum coefficient (impacts stability)
5. **Œ±‚Çú function**: Decay schedule (impacts memory retention)
6. **Œ≥·µ¢ initialization**: Context gate starting values

## Comparison Summary

| Feature | Titans | Miras | Atlas |
|---------|--------|-------|-------|
| Optimization order | 1st (gradient) | 1st (gradient) | 2nd (Muon/NS-5) |
| Memorization unit | Single token | Single token | Context window |
| Decay | Fixed/dynamic | Fixed/dynamic | Dynamic (Œ±‚Çú) |
| Capacity | Linear/polynomial | Linear/polynomial | Polynomial (O(d‚Çñ·µñ)) |
| Memory structure | Linear/MLP | Linear/MLP | Deep MLP + residuals |
| Context handling | Sequential | Sequential | Window-based with gates |
| Test-time adaptation | Yes | Yes | Yes (context-aware) |
| Parallelizable | Limited | Limited | Yes (batch gradients) |

## Key Takeaways for Implementation

1. **Start with deep MLP memory** (‚â•1 hidden layers + residuals)
2. **Implement Muon optimizer** or similar second-order approximation
3. **Use polynomial features** with a·µ¢ = 1/i! initialization
4. **Batch gradient computation** across context window for efficiency
5. **Learn context gates** (Œ≥·µ¢) through backpropagation
6. **Dynamic decay** (Œ±‚Çú) should be input-dependent
7. **Fixed memory footprint** enables long-context deployment
8. **Theoretical capacity scales as O(d‚Çñ·µñ)** - leverage non-linearity

## Open Questions for Implementation

1. **NS-5 details**: Exact algorithm for Normalized Symmetric rank-5 approximation?
2. **Œ≥·µ¢ architecture**: How are context gates computed from inputs?
3. **Œ±‚Çú function**: Exact form of dynamic decay (attention-based? learned MLP?)?
4. **Training stability**: Gradient clipping? Warmup schedule?
5. **Polynomial degree**: Typical p values (2? 3? higher?)?
6. **Context window**: Optimal c for different sequence lengths?

---

**Generated**: 2025-12-08
**Source**: https://arxiv.org/html/2505.23735v1
**Model**: Atlas Memory Architecture

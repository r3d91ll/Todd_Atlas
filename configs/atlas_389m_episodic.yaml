# Atlas 389M Episodic Memory Training Configuration
# Target: H100 GPU with cloud deployment
#
# This configuration implements:
# - Chinchilla-optimal training (~40 tokens/param)
# - Episodic training with storage/retrieval cycles
# - Phase-based gate floor scheduling to prevent collapse
# - Retrieval verification with heavy penalty (5x)
# - Real-time Streamlit monitoring
# - Telegram alerts for critical issues

model:
  d_model: 1024
  n_layers: 16
  n_heads: 8
  d_ff: 4096
  vocab_size: 32100  # T5-base has 32000 base + 100 extra_ids
  max_seq_len: 4096

  # Omega memory config
  d_key: 1024
  d_value: 1024
  poly_degree: 2
  context_window: 16
  init_alpha: 0.99
  init_theta: 0.9
  init_eta: 0.1
  window_size: 512

training:
  # Chinchilla optimal: ~15B tokens for 389M params
  # 15B / (64 * 4096) = ~57K steps
  max_steps: 57000
  learning_rate: 3e-4
  weight_decay: 0.1
  warmup_steps: 2000
  grad_clip: 1.0

  # H100 optimized batch size
  batch_size: 64
  gradient_accumulation_steps: 2
  # Effective batch size: 64 * 2 = 128

  # Logging and checkpointing
  log_interval: 50
  checkpoint_interval: 5000
  metrics_path: "runs/atlas_389m_episodic/metrics_stream.jsonl"
  checkpoint_dir: "runs/atlas_389m_episodic/checkpoints"

  # Device and precision
  device: "cuda"
  dtype: "bfloat16"
  compile_model: true  # Enable torch.compile for H100

  # DDP settings (for multi-GPU if available)
  use_ddp: false  # Set to true for multi-GPU

episodic:
  enabled: true
  storage_samples: 10    # 10 storage samples per episode
  retrieval_samples: 10  # 10 retrieval samples per episode

  # Phase-based gate floor scheduling
  # Phase 1: High floor to establish memory usage
  # Phase 2: Medium floor to allow optimization
  # Phase 3: Low floor to test learned behavior
  phase1_steps: 15000     # Steps 0-15000
  phase2_steps: 25000     # Steps 15000-40000 (cumulative)
  phase1_gate_floor: 0.30
  phase2_gate_floor: 0.10
  phase3_gate_floor: 0.05

  # Gate targets for episodic modes
  storage_gate_target: 0.80   # Force high gates during storage
  retrieval_gate_floor: 0.30  # Minimum gate during retrieval

  # Loss weights
  retrieval_loss_weight: 5.0  # Heavy penalty for retrieval failure
  storage_loss_weight: 1.0    # Standard LM loss during storage

  reset_memory_between_episodes: false

monitoring:
  dashboard_enabled: true
  dashboard_port: 8501

  # Metric streaming for Streamlit
  stream_path: "runs/atlas_389m_episodic/metrics_stream.jsonl"
  flush_interval: 10  # Flush every 10 steps

  # Alert thresholds
  alerts:
    gate_collapse_risk:
      threshold: 0.80
      severity: "critical"
    gate_dead_ratio:
      threshold: 0.30
      severity: "warning"
    retrieval_token_accuracy:
      threshold: 0.50
      severity: "warning"
      direction: "below"
    memory_sparsity:
      threshold: 0.50
      severity: "warning"
    loss:
      threshold: 10.0
      severity: "critical"

  # Telegram notifications
  telegram:
    enabled: true
    # REQUIRED: Set these via environment variables before training:
    #   export TELEGRAM_BOT_TOKEN="your_token"
    #   export TELEGRAM_CHAT_ID="your_chat_id"
    # The script reads from os.environ with these as fallback defaults
    bot_token: ""
    chat_id: ""
    cooldown_minutes: 5  # Prevent alert spam

data:
  # Training data path
  # Fallback behavior: If this path is not found and use_synthetic is not set,
  # the training script will use synthetic data with a warning. Set use_synthetic: true
  # explicitly if you want synthetic data, or ensure this path exists for real data.
  dataset_path: "../datasets/raw/dolma3/dolmino_mix_100B/data/"

  # Categories to use (subset for faster iteration if needed)
  categories:
    - "ingredient1-common_crawl-high-quality_19_science_math_and_technology"
    - "ingredient1-common_crawl-high-quality_19_literature"
    - "ingredient1-code-meta-reasoning"
    - "ingredient1-common_crawl-high-quality_19_history"
    - "ingredient1-common_crawl-high-quality_19_nature"

  # Data loading
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true

  # Tokenization
  tokenizer: "t5-base"  # Must match vocab_size (32100 = T5-base)
  max_seq_len: 4096

# Success criteria for this training run
success_criteria:
  target_perplexity: 200      # Should achieve <200 perplexity
  target_gate_mean: 0.20      # Raw gate mean should stay >20%
  target_collapse_risk: 0.50  # Collapse risk should stay <50%
  target_retrieval_accuracy: 0.90  # During retrieval phases

# Hardware estimates
hardware:
  gpu: "H100"
  vram_required_gb: 40  # Estimated
  expected_time_days: 2  # Approximately 2 days on H100

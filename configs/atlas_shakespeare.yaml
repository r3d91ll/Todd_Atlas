# Atlas-Shakespeare Configuration
# Purpose: Atlas model trained on Shakespeare corpus (English theatrical works)
# Goal: Cross-linguistic comparison with de Vega - independent Kakeya set development
#
# CRITICAL: This model runs INDEPENDENTLY on GPU 0
# NO cross-contamination with de Vega model (GPU 1)
#
# Three-Stage Curriculum:
#   Stage 1: Masked word completion → memory retrieval
#   Stage 2: Manual coherence validation (human-in-the-loop)
#   Stage 3: Memory-augmented generation → creative memory usage
#
# Miras framework choices:
# - Matrix-valued memory (W)
# - L2 attentional bias
# - Local + Global retention gates
# - Momentum-based learning
#
# References:
# - Power et al. (2022): "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
# - Doshi et al. (2025): "Grokking at the Edge of Numerical Stability" (arXiv:2501.04697v2)

model:
  # Scaled down dimensions (~2-3M params after embeddings)
  # MATCHED with de Vega for fair cross-lingual comparison
  d_model: 128
  n_layers: 4
  n_heads: 4
  d_ff: 512
  vocab_size: 29153        # Pruned mT5 tokenizer (88% reduction from 250K)
  max_seq_len: 512

  # Omega memory config (must match d_model per AtlasConfig constraints)
  d_key: 128
  d_value: 128
  poly_degree: 2
  context_window: 16
  init_alpha: 0.99
  init_theta: 0.9
  init_eta: 0.1
  window_size: 128

training:
  max_steps: 100000     # Safety limit - training stops on grokking detection
  learning_rate: 3e-4
  weight_decay: 0.05    # Small regularization for generalization
  warmup_steps: 1000
  grad_clip: 1.0

  batch_size: 48
  gradient_accumulation_steps: 2
  # Effective batch size: 96

  log_interval: 50
  checkpoint_interval: 1000
  metrics_path: "runs/atlas_shakespeare/metrics_stream.jsonl"
  checkpoint_dir: "runs/atlas_shakespeare/checkpoints"

  device: "cuda:0"      # Shakespeare on GPU 0, de Vega on GPU 1
  dtype: "bfloat16"
  compile_model: false

# Curriculum Learning Configuration
curriculum:
  # Stage 1: Memory Retrieval Training (masked word completion)
  stage1:
    enabled: true
    task: "masked_retrieval"
    convergence_window: 2000    # Steps to check for plateau
    convergence_threshold: 0.001  # Loss delta for plateau detection
    checkpoint_prefix: "stage1_converged"

  # Stage 2: Manual Coherence Validation
  stage2:
    enabled: true
    task: "manual_validation"
    n_samples: 100              # Number of prompts for human eval
    coherence_threshold: 0.80   # 80% must be rated coherent
    checkpoint_prefix: "stage2_validated"

  # Stage 3: Memory-Augmented Generation
  stage3:
    enabled: true
    task: "generative_memory"
    max_steps: 50000            # Run until grokking
    memory_query_before_generate: true
    store_generations_in_memory: true
    checkpoint_prefix: "stage3_grokked"

episodic:
  enabled: true
  storage_samples: 5
  retrieval_samples: 5

  # Phase transitions (for Stage 1)
  phase1_steps: 3000     # High floor: steps 0-3000
  phase2_steps: 4000     # Medium floor: steps 3000-7000
  phase1_gate_floor: 0.30
  phase2_gate_floor: 0.10
  phase3_gate_floor: 0.05

  storage_gate_target: 0.80
  retrieval_gate_floor: 0.30

  retrieval_loss_weight: 5.0
  storage_loss_weight: 5.0        # Equal weighting for balanced LM learning

  reset_memory_between_episodes: false

  # Multi-task masked word prediction
  use_masked_retrieval: true
  mask_token_id: 2                # <unk> token ID in pruned mT5 tokenizer
  num_masks: 52                   # 10% of max_seq_len (512), rounded up
  pad_token_id: 0

monitoring:
  dashboard_enabled: true
  dashboard_port: 8501  # Shakespeare dashboard

  # Grokking detection metrics
  grokking:
    enabled: true
    metrics_interval: 500
    track_embeddings: true
    track_memory_matrices: true
    track_hidden_states: true

    # Thresholds for grokking detection (language-only)
    effective_dim_ratio_target: 0.3
    min_gate_mean: 0.1
    min_retrieval_accuracy: 0.5

  # Numerical stability (arXiv:2501.04697v2)
  stability:
    enabled: true
    sc_threshold: 1e-7
    nlm_cosine_threshold: 0.7

    # StableMax: DISABLED - causes embedding collapse
    use_stablemax: false

    # PerpGrad: Testing at reduced strength
    use_orthogonal_grad: true
    orthogonal_grad_strength: 0.5

  alerts:
    gate_collapse_risk:
      threshold: 0.80
      severity: "critical"
    gate_dead_ratio:
      threshold: 0.30
      severity: "warning"
    retrieval_token_accuracy:
      threshold: 0.50
      severity: "warning"
      direction: "below"
    memory_sparsity:
      threshold: 0.50
      severity: "warning"

  telegram:
    enabled: false

data:
  dataset_path: "data/shakespeare"
  tokenizer_path: "tokenizer/atlas_multilingual"
  num_workers: 2
  max_seq_len: 512

# Kakeya extraction settings (for post-training analysis)
kakeya:
  # Target concepts for geometric signature extraction (English)
  concepts:
    - "honour"       # honor (British spelling in Shakespeare)
    - "love"         # love
    - "duty"         # duty
    - "death"        # death
    - "king"         # king
    - "betrayal"     # betrayal
    - "courage"      # courage
    - "revenge"      # revenge
    - "fate"         # fate/destiny
    - "blood"        # blood

  extraction:
    layers: [0, 1, 2, 3]
    extract_memory: true
    extract_hidden: true
    extract_attention: true

# Atlas-Shakespeare Configuration
# Purpose: 10M parameter Atlas model trained on Shakespeare corpus
# Goal: Validate Atlas training + enable Kakeya set extraction for concept geometry
#
# Same Miras framework choices as larger Atlas models:
# - Matrix-valued memory (W)
# - L2 attentional bias
# - Local + Global retention gates
# - Momentum-based learning
#
# Expected training time: 1-2 hours on A6000

model:
  # Scaled down dimensions (~10M params)
  d_model: 256
  n_layers: 4
  n_heads: 4
  d_ff: 1024
  vocab_size: 32000
  max_seq_len: 512  # Shorter for smaller corpus

  # Omega memory config (scaled to d_model)
  d_key: 256
  d_value: 256
  poly_degree: 2
  context_window: 16
  init_alpha: 0.99
  init_theta: 0.9
  init_eta: 0.1
  window_size: 128

training:
  max_steps: 10000      # More steps for smaller corpus
  learning_rate: 3e-4
  weight_decay: 1.0     # Critical for grokking (was 0.1)
  warmup_steps: 500
  grad_clip: 1.0

  batch_size: 32
  gradient_accumulation_steps: 2
  # Effective batch size: 64

  log_interval: 50
  checkpoint_interval: 1000
  metrics_path: "runs/atlas_shakespeare/metrics_stream.jsonl"
  checkpoint_dir: "runs/atlas_shakespeare/checkpoints"

  device: "cuda:1"      # Use A6000 (cuda:0 has other processes)
  dtype: "bfloat16"
  compile_model: false

episodic:
  enabled: true
  storage_samples: 5
  retrieval_samples: 5

  # Phase transitions
  phase1_steps: 3000     # High floor: steps 0-3000
  phase2_steps: 4000     # Medium floor: steps 3000-7000
  phase1_gate_floor: 0.30
  phase2_gate_floor: 0.10
  phase3_gate_floor: 0.05

  storage_gate_target: 0.80
  retrieval_gate_floor: 0.30

  retrieval_loss_weight: 5.0
  storage_loss_weight: 1.0

  reset_memory_between_episodes: false

# Dynamic weight decay (adjusts automatically based on excluded_loss)
dynamic_weight_decay:
  enabled: true
  initial: 1.0          # Starting weight decay (matches training.weight_decay)
  min_value: 0.1        # Minimum allowed
  max_value: 10.0       # Maximum allowed
  adjustment_factor: 1.5  # Multiply by this when adjusting
  patience_steps: 1000    # Steps without progress before adjustment
  excluded_loss_threshold: 0.01  # Minimum change to consider "progress"
  cooldown_steps: 500     # Steps to wait after adjustment

monitoring:
  dashboard_enabled: true
  dashboard_port: 8501

  # Grokking detection metrics
  grokking:
    enabled: true
    metrics_interval: 500        # Compute every N steps
    track_embeddings: true       # Analyze input embeddings
    track_memory_matrices: true  # Analyze memory W matrices
    track_hidden_states: true    # Analyze layer outputs

    # Thresholds for grokking detection
    fourier_concentration_target: 0.5   # Higher = more structured
    circular_fit_target: 0.8            # Higher = more circular
    effective_dim_ratio_target: 0.3     # Lower = more compressed

    # Excluded loss thresholds (PRIMARY grokking indicator)
    excluded_loss_rising_threshold: 0.01   # Trend > this = circuit formation
    excluded_loss_falling_threshold: -0.01 # Trend < this = cleanup phase

    # Frequency ablation for excluded/restricted loss computation
    frequency_ablation:
      enabled: true
      top_k: 10               # Number of key frequencies to analyze
      use_magnitude_ranking: true

  alerts:
    gate_collapse_risk:
      threshold: 0.80
      severity: "critical"
    gate_dead_ratio:
      threshold: 0.30
      severity: "warning"
    retrieval_token_accuracy:
      threshold: 0.50
      severity: "warning"
      direction: "below"
    memory_sparsity:
      threshold: 0.50
      severity: "warning"

  telegram:
    enabled: false  # Disable for local training

data:
  dataset_path: "data/shakespeare"
  num_workers: 2
  max_seq_len: 512

# Kakeya extraction settings (for post-training analysis)
kakeya:
  # Target concepts for geometric signature extraction
  concepts:
    - "honor"
    - "love"
    - "duty"
    - "death"
    - "king"
    - "betrayal"
    - "courage"
    - "revenge"
    - "fate"
    - "blood"

  # Extraction parameters
  extraction:
    # Layer indices to extract from (0-indexed)
    layers: [0, 1, 2, 3]
    # Extract from memory matrices
    extract_memory: true
    # Extract from hidden states
    extract_hidden: true
    # Extract from attention patterns
    extract_attention: true

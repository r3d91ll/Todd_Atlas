# Atlas-Shakespeare Configuration
# Purpose: 10M parameter Atlas model trained on Shakespeare corpus
# Goal: Validate Atlas training + enable Kakeya set extraction for concept geometry
#
# Same Miras framework choices as larger Atlas models:
# - Matrix-valued memory (W)
# - L2 attentional bias
# - Local + Global retention gates
# - Momentum-based learning
#
# Expected training time: 1-2 hours on A6000
#
# References:
# - Power et al. (2022): "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
# - Doshi et al. (2025): "Grokking at the Edge of Numerical Stability" (arXiv:2501.04697v2)

model:
  # Scaled down dimensions (~10M params)
  d_model: 256
  n_layers: 4
  n_heads: 4
  d_ff: 1024
  vocab_size: 32000
  max_seq_len: 512  # Shorter for smaller corpus

  # Omega memory config (scaled to d_model)
  d_key: 256
  d_value: 256
  poly_degree: 2
  context_window: 16
  init_alpha: 0.99
  init_theta: 0.9
  init_eta: 0.1
  window_size: 128

training:
  max_steps: 100000     # Safety limit - training stops on grokking detection
  learning_rate: 3e-4
  weight_decay: 0.05              # Small regularization for generalization
  warmup_steps: 500
  grad_clip: 1.0

  batch_size: 32
  gradient_accumulation_steps: 2
  # Effective batch size: 64

  log_interval: 50
  checkpoint_interval: 1000
  metrics_path: "runs/atlas_shakespeare/metrics_stream.jsonl"
  checkpoint_dir: "runs/atlas_shakespeare/checkpoints"

  device: "cuda:1"      # Use A6000 (cuda:0 has other processes)
  dtype: "bfloat16"
  compile_model: false

episodic:
  enabled: true
  storage_samples: 5
  retrieval_samples: 5

  # Phase transitions
  phase1_steps: 3000     # High floor: steps 0-3000
  phase2_steps: 4000     # Medium floor: steps 3000-7000
  phase1_gate_floor: 0.30
  phase2_gate_floor: 0.10
  phase3_gate_floor: 0.05

  storage_gate_target: 0.80
  retrieval_gate_floor: 0.30

  retrieval_loss_weight: 5.0
  storage_loss_weight: 5.0        # Equal weighting with retrieval for balanced LM learning

  reset_memory_between_episodes: false

  # Multi-task masked word prediction (NEW)
  use_masked_retrieval: true    # Enable masked word prediction for retrieval
  mask_token_id: 4              # Within vocab range (32000), unused token slot
  num_masks: 13                 # ~10% word masking (~13 words per ~130 words in 512 tokens)
  pad_token_id: 0               # Padding token ID (skip when masking)

monitoring:
  dashboard_enabled: true
  dashboard_port: 8501

  # Grokking detection metrics
  grokking:
    enabled: true
    metrics_interval: 500        # Compute every N steps
    track_embeddings: true       # Analyze input embeddings
    track_memory_matrices: true  # Analyze memory W matrices
    track_hidden_states: true    # Analyze layer outputs

    # Thresholds for grokking detection (language-only)
    # Uses masked word accuracy + gate health + effective dimensionality
    effective_dim_ratio_target: 0.3     # Lower = more compressed
    min_gate_mean: 0.1                  # Gate collapse threshold
    min_retrieval_accuracy: 0.5         # Minimum accuracy for grokking

  # Numerical stability (arXiv:2501.04697v2)
  # Detects Softmax Collapse (SC) and Naïve Loss Minimization (NLM)
  # Also provides solutions: StableMax and ⊥Grad optimizer
  stability:
    enabled: true                 # Enable SC/NLM monitoring
    sc_threshold: 1e-7            # Threshold for SC detection
    nlm_cosine_threshold: 0.7     # Cosine threshold for NLM detection

    # StableMax: Numerically stable softmax alternative
    # Prevents floating-point absorption that causes gradient death
    use_stablemax: false  # DISABLED - testing if this causes embedding collapse

    # ⊥Grad: Orthogonal gradient projection
    # Projects out weight-aligned gradients for immediate generalization
    use_orthogonal_grad: false  # DISABLED - any amount causes embedding collapse
    orthogonal_grad_strength: 0.0

  alerts:
    gate_collapse_risk:
      threshold: 0.80
      severity: "critical"
    gate_dead_ratio:
      threshold: 0.30
      severity: "warning"
    retrieval_token_accuracy:
      threshold: 0.50
      severity: "warning"
      direction: "below"
    memory_sparsity:
      threshold: 0.50
      severity: "warning"

  telegram:
    enabled: false  # Disable for local training

data:
  dataset_path: "data/shakespeare"
  num_workers: 2
  max_seq_len: 512

# Kakeya extraction settings (for post-training analysis)
kakeya:
  # Target concepts for geometric signature extraction
  concepts:
    - "honor"
    - "love"
    - "duty"
    - "death"
    - "king"
    - "betrayal"
    - "courage"
    - "revenge"
    - "fate"
    - "blood"

  # Extraction parameters
  extraction:
    # Layer indices to extract from (0-indexed)
    layers: [0, 1, 2, 3]
    # Extract from memory matrices
    extract_memory: true
    # Extract from hidden states
    extract_hidden: true
    # Extract from attention patterns
    extract_attention: true
